{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fddf6e5-83ab-4e39-a662-8976bdb5d796",
   "metadata": {},
   "source": [
    "The goal of this notebook is to build a parity predictor using a RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f7396f14-3fe6-449d-8cfa-0e33b7e87252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch supports CUDA!\n",
      "showing how you have to turn on CUDA to take advantage of it\n",
      "CUDA is turned on False\n",
      "CUDA is turned on True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"PyTorch supports CUDA!\")\n",
    "    rnn = nn.RNN(input_size=10, hidden_size=20, num_layers=1)\n",
    "    print(\"showing how you have to turn on CUDA to take advantage of it\")\n",
    "    print(\"CUDA is turned on\", any(p.is_cuda for p in rnn.parameters()))\n",
    "    rnn.cuda()\n",
    "    print(\"CUDA is turned on\", any(p.is_cuda for p in rnn.parameters()))\n",
    "    # NOTE: you can also turn it on globally with \n",
    "    #torch.cuda.set_device(0)\n",
    "    # but first I want to see how much faster it makes things on my laptop\n",
    "else:\n",
    "    print(\"PyTorch does not support CUDA.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6629d76a-0f22-40f4-86be-14ebcc76987a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import random\n",
    "\n",
    "def generate_parity_data(seq_length=8, batch_size=16, device='cpu'):\n",
    "    \"\"\"\n",
    "    Generate random binary sequences of length `seq_length`.\n",
    "    Return: \n",
    "      x: Tensor of shape [batch_size, seq_length, 1]\n",
    "      y: Tensor of shape [batch_size, seq_length, 1] (the parity at each time step)\n",
    "    \"\"\"\n",
    "    x = torch.randint(0, 2, (batch_size, seq_length, 1), device=device).float()  # 0/1 bits\n",
    "    # Weâ€™ll compute cumulative sums (mod 2) along dimension 1\n",
    "    y = x.cumsum(dim=1) % 2  # shape [batch_size, seq_length, 1]\n",
    "    return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "967b4982-e8a6-41f3-9487-1952c91e2b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [1.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [1.]]])\n",
      "tensor([[[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [1.]]])\n"
     ]
    }
   ],
   "source": [
    "# looks good\n",
    "x,y = generate_parity_data(8, 1)\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "09aecfe5-4ee9-44d7-bf18-49e2a56356ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleParityRNN(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=4, output_size=1, use_cuda=False):\n",
    "        super(SimpleParityRNN, self).__init__()\n",
    "        \n",
    "        # Our mini RNN cell that updates memory each step\n",
    "        self.rnn_cell = nn.RNNCell(input_size, hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        # Move the entire model to CUDA if requested\n",
    "        if use_cuda:\n",
    "            self.cuda()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        # Start memory at zeros on the correct device (CPU or GPU) otherwise it will be on the wrong device and you will get an error\n",
    "        h_t = torch.zeros(batch_size, self.rnn_cell.hidden_size, device=x.device)\n",
    "        \n",
    "        # We'll collect guesses for each time step\n",
    "        outputs = []\n",
    "        for t in range(seq_len):\n",
    "            x_t = x[:, t, :]      # The bit at time t\n",
    "            h_t = self.rnn_cell(x_t, h_t)  # Update memory\n",
    "            out_t = self.fc(h_t)          # Make a guess\n",
    "            outputs.append(out_t.unsqueeze(1))\n",
    "        \n",
    "        # Stack all time-step outputs into one tensor\n",
    "        return torch.cat(outputs, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c604d4fb-e9dc-4890-98f5-da8a6426dcba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def doit(num_epochs, seq_length, batch_size, learning_rate=0.01, use_cuda=False, print_progress=False):\n",
    "    model = SimpleParityRNN(use_cuda=use_cuda)\n",
    "    criterion = nn.BCEWithLogitsLoss()  \n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    accuracy = 0.0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # 1) Generate random data\n",
    "        x, y = generate_parity_data(seq_length, batch_size, device='cuda' if use_cuda else 'cpu')\n",
    "        \n",
    "        # 2) Forward pass\n",
    "        preds = model(x)  # preds shape: [batch_size, seq_length, 1]\n",
    "        \n",
    "        # 3) Compute loss\n",
    "        loss = criterion(preds.view(-1, 1), y.view(-1, 1))\n",
    "        \n",
    "        # 4) Backprop + update\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # --- Measure accuracy occasionally, e.g. every 200 epochs ---\n",
    "        if (epoch+1) % 200 == 0:\n",
    "            # Convert logits to 0/1 predictions\n",
    "            preds_binary = (torch.sigmoid(preds) > 0.5).float()  # shape same as preds\n",
    "            # Compare with ground truth\n",
    "            correct = (preds_binary == y).float().sum().item() \n",
    "            total = y.numel()  # total number of bits predicted\n",
    "            accuracy = correct / total\n",
    "\n",
    "            if print_progress:\n",
    "                print(f\"Epoch {epoch+1}/{num_epochs}, \"\n",
    "                      f\"Loss: {loss.item():.4f}, \"\n",
    "                      f\"Accuracy: {accuracy*100:.2f}%\")\n",
    "    return accuracy\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9e65d784-f672-4798-a990-8521fbfe65c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA: True\n",
      "avg. accuracy 0.7828125 took 129.87962079048157 seconds\n",
      "Using CUDA: False\n",
      "avg. accuracy 0.80390625 took 35.75992679595947 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "for use_cuda in [True, False]:\n",
    "    cur_time = time.time()\n",
    "    print(f\"Using CUDA: {use_cuda}\")\n",
    "    accuracies = [doit(num_epochs = 2000, seq_length = 8, batch_size = 16, use_cuda=use_cuda) for _ in range(10)]\n",
    "    dt = time.time() - cur_time\n",
    "    print(\"avg. accuracy\", sum(accuracies) / len(accuracies), \"took\", dt, \"seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb523f9-63fa-4eb2-a40e-3989386a2c42",
   "metadata": {},
   "source": [
    "I'm seeing 80%+ accuracy across a few different sequences, not bad. can we get closer to 100?\n",
    "\n",
    "As usual, there's a bazillion parameters, plus we're only doing 1 layer. let's just try some variety\n",
    "\n",
    "For timing:\n",
    "* interestingly, with cuda is much slower than without: 80s vs 50s. I'm guessing the overhead here is affecting it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd7ff31-16fc-4faa-a461-233719ff1cf0",
   "metadata": {},
   "source": [
    "First. Let's switch over to nn.RNN instead of nn.RNNCell so we get the forward part just taken care of, and we can add layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "948acdcf-e034-46d1-95ce-0f7a7779f5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParityRNN2(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=4, output_size=1, num_layers=1, use_cuda=False):\n",
    "        super(ParityRNN2, self).__init__()\n",
    "        \n",
    "        # Here, we specify batch_first=True so input is [batch_size, seq_len, input_size]\n",
    "        self.rnn = nn.RNN(input_size=input_size,\n",
    "                          hidden_size=hidden_size,\n",
    "                          num_layers=num_layers,\n",
    "                          batch_first=True)\n",
    "        \n",
    "        # After the RNN processes the sequence, we have hidden_size outputs at each timestep.\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "        # Move the entire model to CUDA if requested\n",
    "        if use_cuda:\n",
    "            self.cuda()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x shape: [batch_size, seq_len, input_size] (because batch_first=True)\n",
    "        \n",
    "        The RNN returns:\n",
    "          outputs: [batch_size, seq_len, hidden_size]\n",
    "          hidden:  [num_layers, batch_size, hidden_size] \n",
    "                   (since num_layers=1, shape is [1, batch_size, hidden_size])\n",
    "        \"\"\"\n",
    "        outputs, hidden = self.rnn(x)\n",
    "        \n",
    "        # outputs has one hidden state vector of length `hidden_size` for each timestep\n",
    "        # We want to turn that into a parity prediction for each timestep:\n",
    "        logits = self.fc(outputs)  # shape: [batch_size, seq_len, output_size]\n",
    "        \n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d6f2fe8b-ea26-4aca-a7d4-d7e8d8f70a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def doit2(num_epochs, seq_length, batch_size, learning_rate=0.01, num_layers=1, use_cuda=False):\n",
    "    model = ParityRNN2(use_cuda=use_cuda, num_layers=num_layers)\n",
    "    criterion = nn.BCEWithLogitsLoss()  \n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    accuracy = 0.0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # 1) Generate random data\n",
    "        x, y = generate_parity_data(seq_length, batch_size, device='cuda' if use_cuda else 'cpu')\n",
    "        \n",
    "        # 2) Forward pass\n",
    "        preds = model(x)  # preds shape: [batch_size, seq_length, 1]\n",
    "        \n",
    "        # 3) Compute loss\n",
    "        loss = criterion(preds.view(-1, 1), y.view(-1, 1))\n",
    "        \n",
    "        # 4) Backprop + update\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # figure out final accuracy            \n",
    "    x, y = generate_parity_data(seq_length, batch_size, device='cuda' if use_cuda else 'cpu')\n",
    "    \n",
    "    # 2) Forward pass\n",
    "    preds = model(x)  # preds shape: [batch_size, seq_length, 1]\n",
    "    \n",
    "    # 3) Compute loss\n",
    "    loss = criterion(preds.view(-1, 1), y.view(-1, 1))\n",
    "        \n",
    "    # Convert logits to 0/1 predictions\n",
    "    preds_binary = (torch.sigmoid(preds) > 0.5).float()  # shape same as preds\n",
    "    # Compare with ground truth\n",
    "    correct = (preds_binary == y).float().sum().item() \n",
    "    total = y.numel()  # total number of bits predicted\n",
    "    accuracy = correct / total\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4dd5b35f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA: True\n",
      "avg. accuracy 0.7265625 took 24.932602167129517 seconds\n",
      "Using CUDA: False\n",
      "avg. accuracy 0.67265625 took 20.379727363586426 seconds\n"
     ]
    }
   ],
   "source": [
    "# apples to apples the last one\n",
    "import time\n",
    "for use_cuda in [True, False]:\n",
    "    cur_time = time.time()\n",
    "    print(f\"Using CUDA: {use_cuda}\")\n",
    "    accuracies = [doit2(num_epochs = 2000, seq_length = 8, batch_size = 16, use_cuda=use_cuda) for _ in range(10)]\n",
    "    dt = time.time() - cur_time\n",
    "    print(\"avg. accuracy\", sum(accuracies) / len(accuracies), \"took\", dt, \"seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce45af5",
   "metadata": {},
   "source": [
    "So using an nn.RNN node vs an nn.RNNCell put cuda at 25s and cpu at 20s. so almost on par. clearly some hidden back and forth with the RNNCell, which makes sense.\n",
    "\n",
    "Next: let's see what happens if we reduce the epochs but make the batch size a lot bigger:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0698a703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA: True\n",
      "avg. accuracy 0.7861114501953125 took 8.18805742263794 seconds\n",
      "Using CUDA: False\n",
      "avg. accuracy 0.793572998046875 took 20.55060124397278 seconds\n"
     ]
    }
   ],
   "source": [
    "# apples to apples the last one\n",
    "import time\n",
    "for use_cuda in [True, False]:\n",
    "    cur_time = time.time()\n",
    "    print(f\"Using CUDA: {use_cuda}\")\n",
    "    accuracies = [doit2(num_epochs = 500, seq_length = 8, batch_size = 4096, use_cuda=use_cuda) for _ in range(10)]\n",
    "    dt = time.time() - cur_time\n",
    "    print(\"avg. accuracy\", sum(accuracies) / len(accuracies), \"took\", dt, \"seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d99ad8",
   "metadata": {},
   "source": [
    "okay 6s on cuda vs 22s on GPU, but the accuracy is balls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6553456e",
   "metadata": {},
   "source": [
    "# Wrapup\n",
    "\n",
    "* Good RNN refresher. \n",
    "* make sure to take GPU parallelism into account\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886bf057",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
